### PCA

主成分分析，一种使用广泛的数据降维方式。
考虑将n维特征映射到k维上

pca需要求协方差矩阵的特征值和特征向量，因为协方差矩阵是实对称矩阵，且能保证转换后的维度上方差都取最大值。不同维度之间协方差为零 没有相关性。   


什么是矩阵相似：
存在可逆矩阵 满足 B = P<sup>-1</sup>AP,则A和B是相似的   
相似矩阵是同一个线性变换在不同基上的描述，这是相似变换的几何意义   
将矩阵变成对角矩阵后，描述的依然是同一个线性变换，对角矩阵有很好的性质，方便运算。   
矩阵对角化在控制工程和机械振动领域有将复杂方程解耦的作用，简化计算。   
一个矩阵如果不能对角化 也可以变化成jordan标准型

PCAWhitening: PCA白化将数据变成独立同分布，特征之间独立（高斯分布的不相关），并且分布一致   
在PCA中 由于计算协方差矩阵的特征分解 所以获得的特征向量也就是基一定是正交的，但是只取
前k大的特征值，由于协方差矩阵的特征值代表了主成分的方差大小，所以pca只是让新维度之间独立，
在whitening步骤中让特征数据同分布

pca的证明 可以使用拉格朗日乘子法
非线性降维算法有核pca，神经网络(自动编码器) 流形学习


### factor analysis(因子分析)
在因子分析中，我们将变量按相关性分组，特定组内所有变量相关性较高，组间变量
的相关性较低，我们把每一组称为一个因子，他是多个变量的组合。和原始数据集的变量
相比，这些因子在数量上更少但携带的基本信息一致。


### ICA
PCA是一个降维过程，但是ICA则是帮助你从多个维度分离有用数据的过程
PCA认为主元正交，样本呈现高斯分布；独立成分分析不需要样本呈高斯分布
独立是值 数学期望里面两变量满足:   
E(xy) = E(x)E(y)   
独立和不相关的区别：   
独立是指两个变量没有关系
而不相关是指没有线性关系



### MDS
多维标度法 是一种在低维空间展示距离数据结构的多元数据分析技术
当n个对象（object）中各对对象之间的相似性（或距离）给定时，
确定这些对象在低维空间中的表示，并使其尽可能与原先的相似性（或距离）“大体匹配”，
使得由降维所引起的任何变形达到最小

### SVD
对于不能进行特征值的分解的非方阵矩阵可以使用SVD分解:   
A = Udiag(lambda)V<sup>T</sup>

其中U是A<sup>T</sup>A方阵的正交向量组成的矩阵
而V是AA<sup>T</sup>方阵的正交向量



### LDA

### LLE
假设数据在局部是线性的
可以将线性不可分的问题变成低维度上的线性可分
https://www.cnblogs.com/jiangxinyang/p/9314256.html
　主要优点：

　　1）可以学习任意维的局部线性的低维流形。

　　2）算法归结为稀疏矩阵特征分解，计算复杂度相对较小，实现容易。

　　3）可以处理非线性的数据，能进行非线性降维。

　　主要缺点：　　

　　1）算法所学习的流形只能是不闭合的，且样本集是稠密的。

　　2）算法对最近邻样本数的选择敏感，不同的最近邻数对最后的降维结果有很大影响。



### ISOMAP 等距离映射
数据降维后保持测地距离不变，找到样本的全局最优解，


### TSNE



### laplacian eigenmaps
https://zhuanlan.zhihu.com/p/25096844

### kernel pca

黎曼几何
https://zhuanlan.zhihu.com/p/19879691
