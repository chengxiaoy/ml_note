### PCA
https://zhuanlan.zhihu.com/p/77151308
主成分分析，一种使用广泛的数据降维方式。
考虑将n维特征映射到k维上

pca需要求协方差矩阵的特征值和特征向量，因为协方差矩阵是实对称矩阵，且能保证转换后的维度上方差都取最大值。不同维度之间协方差为零 没有相关性。   


什么是矩阵相似：
存在可逆矩阵 满足 B = P<sup>-1</sup>AP,则A和B是相似的   
相似矩阵是同一个线性变换在不同基上的描述，这是相似变换的几何意义   
将矩阵变成对角矩阵后，描述的依然是同一个线性变换，对角矩阵有很好的性质，方便运算。   
矩阵对角化在控制工程和机械振动领域有将复杂方程解耦的作用，简化计算。   
一个矩阵如果不能对角化 也可以变化成jordan标准型

PCAWhitening: PCA白化将数据变成独立同分布，特征之间独立（高斯分布的不相关），并且分布一致   
在PCA中 由于计算协方差矩阵的特征分解 所以获得的特征向量也就是基一定是正交的，但是只取
前k大的特征值，由于协方差矩阵的特征值代表了主成分的方差大小，所以pca只是让新维度之间独立，
在whitening步骤中让特征数据同分布

pca依据最小重构误差证明 可以使用拉格朗日乘子法
非线性降维算法有核pca，神经网络(自动编码器) 流形学习


### factor analysis(因子分析)
在因子分析中，我们将变量按相关性分组，特定组内所有变量相关性较高，组间变量
的相关性较低，我们把每一组称为一个因子，他是多个变量的组合。和原始数据集的变量
相比，这些因子在数量上更少但携带的基本信息一致。


### ICA
PCA是一个降维过程，但是ICA则是帮助你从多个维度分离有用数据的过程
PCA认为主元正交，样本呈现高斯分布；独立成分分析不需要样本呈高斯分布
独立是值 数学期望里面两变量满足:   
E(xy) = E(x)E(y)   
独立和不相关的区别：   
独立是指两个变量没有关系
而不相关是指没有线性关系



### MDS
多维标度法 是一种在低维空间展示距离数据结构的多元数据分析技术
当n个对象（object）中各对对象之间的相似性（或距离）给定时，
确定这些对象在低维空间中的表示，并使其尽可能与原先的相似性（或距离）“大体匹配”，
使得由降维所引起的任何变形达到最小   
https://blog.csdn.net/csdn_inside/article/details/86004733   
具体做法是 根据距离矩阵D 求出新空间的内积矩阵B，再由内积矩阵B求得新空间的表达方法Z
求得B后
![mds](https://img-blog.csdnimg.cn/20190107153952374.png)

pca和msd并不能很好将流行数据线性可分


### SVD
对于不能进行特征值的分解的非方阵矩阵可以使用SVD分解:   
A = Udiag(lambda)V<sup>T</sup>

其中U是A<sup>T</sup>A方阵的正交向量组成的矩阵
而V是AA<sup>T</sup>方阵的正交向量



### LDA

## **_manifold learning_**
![test](https://user-images.githubusercontent.com/19379550/62920393-37e74600-bdd8-11e9-86e9-de5bc3050788.jpg)
https://blog.csdn.net/zhulingchen/article/details/2123129   
线性降维算法通过线性变换 将样本投影到低维空间中。
对非线性数据有局限性（什么是非线性数据，有什么局限性）

非线性映射
流形学习（manifold learning）假设数据在高维空间的分布位于某一更低维的流形上，基于这个假设来进行数据的分析。    
对于降维，要保证降维之后的数据同样满足与高维空间流形有关的几何约束关系。除此之外，流形学习还可以用实现聚类，分类以及回归算法


### LLE
假设数据在局部是线性的
可以将线性不可分的问题变成低维度上的线性可分
https://www.cnblogs.com/jiangxinyang/p/9314256.html
两个步骤：
1. 根据邻域计算权重矩阵
![1](https://images2018.cnblogs.com/blog/1335117/201807/1335117-20180715173151683-1713942804.png)
2. 根据领域重构系数不变，去求每个样本在低维空间的坐标   
![2](https://images2018.cnblogs.com/blog/1335117/201807/1335117-20180715173335979-1637229042.png)   
利用M矩阵   
![3](https://images2018.cnblogs.com/blog/1335117/201807/1335117-20180715173416202-1101911950.png)   
　主要优点：

　　1）可以学习任意维的局部线性的低维流形。

　　2）算法归结为稀疏矩阵特征分解，计算复杂度相对较小，实现容易。

　　3）可以处理非线性的数据，能进行非线性降维。

　　主要缺点：　　

　　1）算法所学习的流形只能是不闭合的，且样本集是稠密的。

　　2）算法对最近邻样本数的选择敏感，不同的最近邻数对最后的降维结果有很大影响。



### ISOMAP 等距离映射
数据降维后保持测地距离不变，找到样本的全局最优解    
流形具有在局部与欧式空间同胚的空间，也就是它在局部具有欧式空间的性质，能用欧式距离来进行距离计算。这就给降维带来了很大的启发，
若低维流形嵌入到了高维空间，此时样本在高维空间的分布虽然复杂，但在局部上仍具有欧式空间的性质，因此可以在局部建立降维映射关系，然后再设法将局部映射关系推广到全局。
在高维空间使用测地距离，而测地距离是由局部的欧式距离的最短路径确定的

![ISOMAP](https://images2018.cnblogs.com/blog/1335117/201807/1335117-20180715172613023-519652121.png)

### TSNE

SNE(stochastic neighbor embedding)
在高维空间相似的数据点，映射到低维空间距离也是相似的。通常使用欧式距离来描述这种相似性，而SNE把这种距离关系转换为一种条件概率来表示相似性。
高维空间中的两个数据点xi和xj，xi以条件概率Pj|i选择xj作为它的邻近点。考虑以xi为中心点的高斯分布，若xj越靠近xi，则Pj|i越大。反之若两者相距较远，则Pj|i极小。

如果降维后的yi 和yj反应了高维数据的分布，那么条件概率应该相等，而判断两个分布之间的相似性，需要使用KL散度（Kullback-Leibler Divergence）
http://bindog.github.io/blog/2016/06/04/from-sne-to-tsne-to-largevis/#top



Why is t-SNE not used as a dimensionality reduction technique for clustering or classification?
https://stats.stackexchange.com/questions/340175/why-is-t-sne-not-used-as-a-dimensionality-reduction-technique-for-clustering-or

原因在与 tsne不能学习到一个高维数据到低维数据的映射,同时tsne降维后不保留距离信息只保留概率分布，所以后续只能使用非距离的
聚类算法，而如果要使用分类的化，可以使用autoencoder autoencoder可以看作是有参数的tsne，可以学习到空间映射。

同时tsne, As the global distances are lost.





### laplacian eigenmaps
https://zhuanlan.zhihu.com/p/25096844

graph laplacian 
W 是邻接矩阵(adjacency matrix)
D 是度矩阵(degree of vertex)

graph laplacian: L = D-W
矩阵L是 半正定的 证明很简单，通过定义分解L可证明特征值都大于等于零
![semi positive](https://pic4.zhimg.com/80/v2-6dfaf220b8c04540a0a7a1f5b48f8083_hd.png)

**Laplacian Eigenmaps**

面对 N 个维度为 m 的样本,希望可以把它们降到 k 维(k<m)。不过首先我们要把样本们转化成一张图。一般来说有两种思路可以完成这一步:    
1. 对于每个顶点,找到距离它最近的 k 个邻居, 每个邻居用下面这个高斯函数计算边的权重;     
2. 以每个顶点为中心画一个半径为 R 的球, 所有在这个球里面的点都是邻居,用下面这个高斯函数计算一下权重。

![weight](https://pic4.zhimg.com/80/v2-40cfb60b3bd75dbc4f32d447d614fe0b_hd.png)

于是就有了一个 graph,然后就可以用 Laplacian 三板斧了: 1. 计算 L; 2. 把 L 做SVD分解; 3. 找 出 top k+1 最小的 eigenvalue, 去掉为 0 的那个,
就能得到对应的 k 个维度为 1*N 的向量, 变成 N 个 k*1 的向量就是降维之后的点。

我们的目的是让相似的数据样例i和j在降维后的目标子空间里仍旧尽量接近



### kernel pca

黎曼几何
https://zhuanlan.zhihu.com/p/19879691
